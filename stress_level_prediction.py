# -*- coding: utf-8 -*-
"""stress_level_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ScjUxT5M7Tbn_Mn4dDx3zCb24l4NFnqF
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

import tensorflow as tf
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers

# Load data
data = pd.read_csv('C:\\Users\\devik_k\\OneDrive\\Documents\\projects\\SLP\\stress_data.csv')
data.head()

"""ANALYZING THE DATA"""

data.shape  # returns a tuple of array dimension that specifies the number of rows and columns

data.info() # prints the information about the dataframe

data.describe()

"""DATA CLEANING"""

#Analyze the numerical and categorical features, and convert categorical feature into numerical.
data['Stress_Level'].unique()

#Check for missing values and handle them.
data.isnull().sum()

#Check for duplicate values
data.duplicated().sum()

"""EDA - EXPLORATORY DATA ANALYSIS

Univariate Non-Graphical
"""

#Check for outliers.
data.skew()

plt.figure(figsize=(4,4))
plt.boxplot(data)
plt.show()

#All 4 coulumns contains skews
#Skewness Reduction
# Logarithmic Transformation
hum_log = np.log(data['Heart rate'])
h = round(hum_log.skew(),10)
print(h)

# SquareRoot Transformation
temp_sqrt = np.sqrt(data['Temperature'])
temp_sqrt.skew()
t = round(temp_sqrt.skew(),10)
print(t)

quantile1=data["Step_count"].quantile(0.25)
quantile2=data["Step_count"].quantile(0.75)

quantile1

quantile2

data["Step_count"]=np.where(data["Step_count"]<quantile1,quantile1,data["Step_count"])
data["Step_count"]=np.where(data["Step_count"]>quantile2,quantile2,data["Step_count"])

s = round(data['Step_count'].skew(),10)
print(s)

# SquareRoot Transformation
stress_sqrt = np.sqrt(data['Stress_Level'])
sl = round(stress_sqrt.skew(),10)
print(sl)

"""Univariate Graphical"""

column=['Step_count','Heart rate']
for category in column:
    plt.figure(figsize=(3,3))
    plt.hist(data[category])
    plt.title(category)
    plt.show()

# histplot (categorical)
plt.figure(figsize=(3,3))
sns.set(font_scale=1)
sns.histplot(data=data, x='Stress_Level')

plt.figure(figsize=(3,3))
sns.boxplot(data['Temperature'])

"""Multivariate Non-Graphical"""

#Correlation

correlation = data.corr()
correlation

plt.figure(figsize=(3,3))
sns.heatmap(correlation,annot=True,cmap='crest',linewidths=0.2)
plt.show()

"""Multivariate Graphical"""

plt.figure(figsize=(3,3))
sns.scatterplot(x='Heart rate',y='Temperature',hue='Stress_Level',data=data)
plt.show()

"""MODELLING"""

from sklearn.model_selection import train_test_split
X=data.drop(['Stress_Level'],axis=1)
y=data['Stress_Level']
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)

"""LOGISTIC REGRESSION"""

from sklearn.linear_model import LogisticRegression
regressor = LogisticRegression(C=1.0,random_state=2)
regressor.fit(X_train,y_train)

from sklearn.metrics import accuracy_score, confusion_matrix
prediction = regressor.predict(X_test)
confusionmatrix = confusion_matrix(y_test,prediction)
print(confusionmatrix)

print(accuracy_score(y_test,prediction))

"""SUPPORT VECTOR MACHINE"""

from sklearn.svm import SVC
classifier = SVC(kernel='linear', random_state=0)
classifier.fit(X_train, y_train)

y_predict= classifier.predict(X_test)
score=accuracy_score(y_test,y_predict)
print(score)

"""NEURAL NETWORK"""

# Encode labels
encoder = LabelEncoder()
data['Stress_Level'] = encoder.fit_transform(data['Stress_Level'])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data.drop(['Stress_Level'], axis=1), data['Stress_Level'], test_size=0.3)

# Define model architecture
model = tf.keras.Sequential([
    layers.Dense(64, activation='relu', input_shape=[len(X_train.keys())]),
    layers.Dense(64, activation='relu'),
    layers.Dense(3, activation='softmax')
])

# Compile model
model.compile(loss='sparse_categorical_crossentropy',
              optimizer=tf.keras.optimizers.Adam(),
              metrics=['accuracy'])

# Train model
model.fit(X_train, y_train, epochs=150, batch_size=10)

# Evaluate model
test_loss, test_acc = model.evaluate(X_test, y_test)

# Make predictions on new data
new_data = np.array([[90.89, 61.84, 0.0206306]])
prediction = model.predict(new_data)
prediction
predicted_class = encoder.inverse_transform([np.argmax(prediction)])

# Print predicted stress level
print(f"Predicted stress level: {predicted_class}")

if predicted_class == [0]:
  print("Low Stress level")
elif predicted_class == [1]:
  print("Normal Stress level")
elif predicted_class == [2]:
  print("High Stress level")

import pickle

# Saving model to disk
pickle.dump(model, open('model.pkl','wb'))